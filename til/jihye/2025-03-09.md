### 한것
강의
- 자연언어 처리의 역사  
~ Attenion이란 무엇인가?

LLM 책
- 트랜스포머 아키텍처 살펴보기
### 배운것
#### 딥러닝 모델의 언어 학습법
1.1 딥러닝과 언어 모델링

- 언어모델 - 다음에 올 단어를 예측하는 모델
- 임베딩 - 데이터의 의미와 특징을 숫자로 표현한 것
- 워드투벡터 - 단어를 의미를 담아 숫자 임베딩으로 표현하는 모델
- 다운스트림 - 사전학습을 미세조정해 풀고자 하는 과제
- 새로운 문제만으로 학습한 모델보다, 사전학습된 데이터로 전이학습을 진행하는게 일반적으로 성능이 더 높다. 일반적인 특징을 파악하는 능력이 같기때문

2.1 트랜스포머 아키택처

- RNN - 이전 토큰의 출력을 다시 모델의 입력으로 순차적으로 처리  
단점 - 학습이 느리고 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되며 성능이 떨어짐
- 트랜스포머의 장점 - 깊은 모델도 학습이 잘됨, 확장이 용이, 병렬연산 가능, 입력이 길어도 됨
- 셀프어텐션 - 입력된 문장의 각 단어의 관련을 계산

인코더: 입력 -> 토큰화 -> 위치인코딩 -> 정규화 -> 어텐션 -> 정규화 -> 피드포워드

디코더: ~ + 마스트 어텐션 + ~ + 크로스어텐션 + ~ + 소프트맥스

토큰화  
- 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여  
- 큰단위(단어)로 토큰화 하면 의미가 잘 유지되지만 사전의 크기가 커짐, OOV(out of vocabulary) 문제도 발생
- 서브워드 - 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정, 의미를 유지하며 사전의 크기를 효율적으로

임베딩
- 토큰임베딩 - 토큰을 임베딩
- 위치임베딩 - 텍스트의 순서정보를 기억하기 위해
    - 절대적 위치 임베딩
    - 상대적 위치 임베딩

어텐션
- 쿼리, 키, 값을 이용해 단어 사이의 직접적인 관계를 계산한다.
- 간접적인 관련성을 찾기위해 가중치를 부여한다.
- 멀티헤드어텐션 - 여러 어텐션 연상을 동시에 적용

층 정규화  
- 이미지 처리에는 배치 정규화를 사용하지만, 언어처리에서는 입력의 길이가 다양하므로, 층 정규화를 한다.
- 사후정규화
- 사전정규화 - 더 안정적이라 주로 사용

피드포워드  
- 데이터의 특징을 학습하는 완전연결층(단어가 아닌 텍스트 전체)
- 선형층, 드롭아웃층, 층정규화, 활성함수로 구성
