### 한것
Advanced
- BERT 이전 몰아보기 ~ Tokenizer 이해하기

### 배운것
#### BERT 이전 몰아보기
Seq2Seq
- 기계번역 (Neural machine translation, NMT)
- 인코더 - 입력 문장을 고정된 길이의 벡터로 변환
- 디코더 - 고정된 길이의 벡터 입력을 출력 문장으로 변환

RNN (Recurrent Neural Network)
- 입력 시퀀스을 단계마다 순차적으로 학습

LSTM (Long-Short-Term-Memory)
- RNN 의 단점인 장기 의존성 문제를 보완
- cell state 에 핵심 정보들을 모두 담아두고, 필요한 정보만을 hidden state 에 업데이트

GRU(Gated Recurrent Unit)
- LSTM 모델 구조를 더 단순화
- 1가지 상태 (state)와 2가지 게이트 (gate)
- 장기 의존성 문제에는 LSTM 보다 취약

RNN + Attention
- 디코더의 hidden state 가 인코더의 각 스텝의 hidden state 중 어느것에 유사한 지 계산

Transformer
- Self-Attention
    - 같은 문장 내의 토큰들끼리 어텐션
- Multi-head Attention
    - 토큰 간의 다양한 종속성을 학습하기 위해, Attention을 여러 개의 Head 로 나눠서 병렬로 계산
- Query,Key,Value 연산
    - //복습필요
- Positional Encoding
    - 시퀀스의 단어들의 순서 정보
    - Input embedding 값에 더한다

#### 자연어 처리 평가지표
- Confusion matrix 복습
- BLEU (Bilingual Evaluation Understudy Score)
    - n-gram(n = 1~4)의 n값에 따라 문장을 나눠서 정답값과 겹치는 지 평가
    - brevity penalty
        - reference 문장보다 짧은 예측 문장을 내놓을 시, 1 이하의 값 precision 값을 보정

#### 자연어 처리 Pipeline
1. 라이브러리 및 데이터셋 로드
2. 데이터셋 구축
    - train/valid split
    - 토크나이징 (Tokenizing)
    - torch dataset class 로 변환
3. 모델 및 토크나이저 가져오기
4. 모델 학습
5. 추론 및 평가

### 할것
ELMo와 BERT 복습
